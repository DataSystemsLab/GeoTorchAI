{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44609e60",
   "metadata": {},
   "source": [
    "# This Example shows the Segmentation of Cloud38 satellite images using the deep learning model UNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5dacf",
   "metadata": {},
   "source": [
    "Find the details of the DeepSAT-V2 model in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28\">corresponding paper</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b201b",
   "metadata": {},
   "source": [
    "Find the details of the dataset <a href=\"https://www.kaggle.com/datasets/sorour/38cloud-cloud-segmentation-in-satellite-images\">here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5249ddba",
   "metadata": {},
   "source": [
    "### Import Modules and Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c2fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from geotorchai.models.raster import UNet\n",
    "from geotorchai.datasets.raster import Cloud38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09752ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define parameters\n",
    "epoch_nums = 5\n",
    "learning_rate = 0.0002\n",
    "batch_size = 8\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed = int(time.time())\n",
    "params = {'batch_size': batch_size, 'shuffle': False}\n",
    "\n",
    "## make sure that PATH_TO_DATASET exists in the running directory\n",
    "PATH_TO_DATASET = \"data/38-Cloud_training\"\n",
    "MODEL_SAVE_DIR = \"model-unet\"\n",
    "MODEL_SAVE_PATH = MODEL_SAVE_DIR + \"/unet.pth\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e8be3",
   "metadata": {},
   "source": [
    "### Load Data and Add Normalization Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d331b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanchan/.pyenv/versions/3.11.0/lib/python3.11/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "## load data and calculate mean and std to perform normalization transform\n",
    "## Set download=True if dataset is not available in the given path\n",
    "fullData = Cloud38(root = PATH_TO_DATASET)\n",
    "\n",
    "full_loader = DataLoader(fullData, batch_size= batch_size)\n",
    "channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "for i, sample in enumerate(full_loader):\n",
    "    data_temp, _ = sample\n",
    "    channels_sum += torch.mean(data_temp, dim=[0, 2, 3])\n",
    "    channels_squared_sum += torch.mean(data_temp**2, dim=[0, 2, 3])\n",
    "    num_batches += 1\n",
    "\n",
    "mean = channels_sum / num_batches\n",
    "std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52141dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the transform operation\n",
    "sat_transform = transforms.Normalize(mean, std)\n",
    "## Load data with desired transformation and additional handcrafted features enabled\n",
    "fullData = Cloud38(root = PATH_TO_DATASET, transform = sat_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd30478",
   "metadata": {},
   "source": [
    "### Split Dataset into Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd23a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize training and validation indices to split the dataset\n",
    "dataset_size = len(fullData)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e15e039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define training and validation data sampler\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "## Define training and validation data loader\n",
    "train_loader = DataLoader(fullData, **params, sampler=train_sampler)\n",
    "val_loader = DataLoader(fullData, **params, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b680b282",
   "metadata": {},
   "source": [
    "### Initialize Model and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ffa278d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set device to CPU or GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "## Define Model\n",
    "model = UNet(4, 2)\n",
    "## Define hyper-parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.to(device)\n",
    "loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb5f439",
   "metadata": {},
   "source": [
    "### Method for Returning Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a79a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before starting training, define a method to calculate validation accuracy\n",
    "def get_validation_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_sample = 0\n",
    "    running_acc = 0.0\n",
    "    for i, sample in enumerate(data_loader):\n",
    "        inputs, labels = sample\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        running_acc += (predicted == labels).float().mean().item()*len(labels)\n",
    "        total_sample += len(labels)\n",
    "\n",
    "    accuracy = 100 * running_acc / total_sample\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaabcc32",
   "metadata": {},
   "source": [
    "### Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e65c96ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training Loss: 0.6838\n",
      "Validation Accuracy:  94.57973837852478 %\n",
      "Best model saved!\n",
      "Epoch [2/5], Training Loss: 0.6377\n",
      "Validation Accuracy:  93.67099404335022 %\n",
      "Epoch [3/5], Training Loss: 0.6637\n",
      "Validation Accuracy:  93.34920048713684 %\n",
      "Epoch [4/5], Training Loss: 0.5927\n",
      "Validation Accuracy:  92.69375205039978 %\n",
      "Epoch [5/5], Training Loss: 0.5643\n",
      "Validation Accuracy:  91.90165400505066 %\n"
     ]
    }
   ],
   "source": [
    "## Perform training and validation\n",
    "max_val_accuracy = None\n",
    "for e in range(epoch_nums):\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        inputs, labels = sample\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Training Loss: {:.4f}'.format(e + 1, epoch_nums, loss.item()))\n",
    "\n",
    "    ## Perform model validation after finishing each epoch training\n",
    "    val_accuracy = get_validation_accuracy(model, val_loader, device)\n",
    "    print(\"Validation Accuracy: \", val_accuracy, \"%\")\n",
    "\n",
    "    if max_val_accuracy == None or val_accuracy > max_val_accuracy:\n",
    "        max_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print('Best model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2a42f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
