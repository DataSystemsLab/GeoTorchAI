{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f886bff",
   "metadata": {},
   "source": [
    "# This Example shows how to classify EuroSAT satellite images using the deep learning model DeepSAT-V2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4865b7",
   "metadata": {},
   "source": [
    "### EuroSAT satellite dataset contains images from 10 different classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba602b72",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Annual Crop</li>\n",
    "<li>Forest</li>\n",
    "<li>Herbaceous Vegetation</li>\n",
    "<li>Highway</li>\n",
    "<li>Industrial</li>\n",
    "<li>Pasture</li>\n",
    "<li>Permanent Crop</li>\n",
    "<li>Residential</li>\n",
    "<li>River</li>\n",
    "<li>SeaLake</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b925031",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14eaf5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from geotorchai.models.raster import SatCNN\n",
    "from geotorchai.preprocessing.torch_df import RasterClassificationDf\n",
    "from geotorchai.preprocessing import SparkRegistration, load_geotiff_image_as_binary_data\n",
    "from geotorchai.preprocessing.raster import RasterProcessing as rp\n",
    "from geotorchai.utility import TorchAdapter\n",
    "\n",
    "# Import Apache Sedona\n",
    "from sedona.spark import *\n",
    "\n",
    "## Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf, expr, array, concat\n",
    "\n",
    "\n",
    "## Import distributed modules\n",
    "from torch.utils.data import DistributedSampler, DataLoader\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "from petastorm import TransformSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd00c43",
   "metadata": {},
   "source": [
    "## Define spark session and Register with GeoTorchAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae968edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/20 14:14:57 WARN Utils: Your hostname, Kanchans-Laptop.local resolves to a loopback address: 127.0.0.1; using 192.168.1.4 instead (on interface en0)\n",
      "23/07/20 14:14:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/kanchan/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/kanchan/.ivy2/jars\n",
      "org.apache.sedona#sedona-spark-shaded-3.0_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e96c1194-cfb6-4a8b-bc8b-ab13ead57951;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/kanchan/program_files/spark-3.4.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.sedona#sedona-spark-shaded-3.0_2.12;1.4.1 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.4.0-28.2 in central\n",
      ":: resolution report :: resolve 84ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.sedona#sedona-spark-shaded-3.0_2.12;1.4.1 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.4.0-28.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e96c1194-cfb6-4a8b-bc8b-ab13ead57951\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n",
      "23/07/20 14:14:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "config = SedonaContext.builder().master(MASTER_URL).config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.4_2.12:1.4.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.4.0-28.2').getOrCreate()\n",
    "\n",
    "spark = SedonaContext.create(config)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33602c9",
   "metadata": {},
   "source": [
    "### Register SparkSession with GeoTorchAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f70e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkRegistration.set_spark_session(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a562ad1",
   "metadata": {},
   "source": [
    "## Load Raster Data to Sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e8033c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:=======================================>              (181 + 12) / 246]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+\n",
      "|                path|    modificationTime|length|             content|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "|file:/Users/kanch...|2023-04-28 09:36:...|107244|[49 49 2A 00 08 0...|\n",
      "|file:/Users/kanch...|2023-04-28 09:36:...|107244|[49 49 2A 00 08 0...|\n",
      "|file:/Users/kanch...|2023-04-28 09:36:...|107244|[49 49 2A 00 08 0...|\n",
      "|file:/Users/kanch...|2023-04-28 09:36:...|107244|[49 49 2A 00 08 0...|\n",
      "|file:/Users/kanch...|2023-04-28 09:36:...|107244|[49 49 2A 00 08 0...|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raster_df = load_geotiff_image_as_binary_data(\"data/eurosat_total/*.tif\")\n",
    "raster_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7187e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_df = rp.get_array_from_binary_raster(raster_df, 13, \"content\", col_new_array_data=\"array_data\")\n",
    "raster_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23aba9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- raster_data: raster (nullable = true)\n",
      " |-- array_data: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raster_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65fc23d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(file_path):\n",
    "    return file_path.split(\"/\")[-1].split(\"_\")[0]\n",
    "\n",
    "get_name_udf = udf(lambda x: get_name(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a575cbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------+\n",
      "|                path|    modificationTime|length|             content|         raster_data|          array_data|  category|\n",
      "+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------+\n",
      "|file:/Users/kanch...|2023-04-28 09:36:...|107244|[49 49 2A 00 08 0...|GridCoverage2D[\"g...|[1420.0, 1420.0, ...|AnnualCrop|\n",
      "|file:/Users/kanch...|2023-04-28 09:36:...|107244|[49 49 2A 00 08 0...|GridCoverage2D[\"g...|[1133.0, 1133.0, ...|AnnualCrop|\n",
      "|file:/Users/kanch...|2023-04-28 09:36:...|107244|[49 49 2A 00 08 0...|GridCoverage2D[\"g...|[1773.0, 1773.0, ...|AnnualCrop|\n",
      "|file:/Users/kanch...|2023-04-28 09:36:...|107244|[49 49 2A 00 08 0...|GridCoverage2D[\"g...|[1388.0, 1388.0, ...|AnnualCrop|\n",
      "|file:/Users/kanch...|2023-04-28 09:36:...|107244|[49 49 2A 00 08 0...|GridCoverage2D[\"g...|[1650.0, 1650.0, ...|AnnualCrop|\n",
      "+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raster_df = raster_df.withColumn(\"category\", get_name_udf(raster_df[\"path\"]))\n",
    "raster_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9247b",
   "metadata": {},
   "source": [
    "## Visualize the Bands of an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_index = 5\n",
    "rp.visualize_all_bands(raster_df, \"array_data\", img_index, no_bands=13, height=64, width=64, axis_rows=2, axis_cols=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90630909",
   "metadata": {},
   "outputs": [],
   "source": [
    "objRasterClassify = RasterClassificationDf(raster_df, \"array_data\", \"category\", 64, 64, 13)\n",
    "formatted_df = objRasterClassify.get_formatted_df()\n",
    "transform_spec = objRasterClassify.get_transform_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c07da9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- img_data: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7afe57e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanchan/.pyenv/versions/3.11.0/lib/python3.11/site-packages/petastorm/spark/spark_dataset_converter.py:28: FutureWarning: pyarrow.LocalFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  from pyarrow import LocalFileSystem\n"
     ]
    }
   ],
   "source": [
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, \"file:///tmp/petastorm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "651fc9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanchan/.pyenv/versions/3.11.0/lib/python3.11/site-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  self._filesystem = pyarrow.localfs\n",
      "Converting floating-point columns to float32\n",
      "The median size 2118207 B (< 50 MB) of the parquet files is too small. Total size: 20620744 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///tmp/petastorm/20230720141644-appid-local-1689887698493-cc88d6ce-9f17-47fb-b38e-db259aff486a/part-00002-6ef584f9-e0b5-4c2e-b1fb-3066f656407e-c000.parquet, ...\n"
     ]
    }
   ],
   "source": [
    "converter_train = make_spark_converter(formatted_df)\n",
    "print(len(converter_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c9ccc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0002\n",
    "batch_size = 8\n",
    "epoch_nums = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1747cbb1",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "Accuracy will be very low since training is performed on a small dataset for only 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee76c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataloader_iter, steps_per_epoch, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_data = next(train_dataloader_iter)\n",
    "        inputs = batch_data['image_data']\n",
    "        labels = batch_data['label']\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        with torch.set_grad_enabled(True):\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "                \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a46ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn.to(device)\n",
    "    with converter_train.make_torch_dataloader(transform_spec=transform_spec, batch_size=batch_size) as train_dataloader:\n",
    "        steps_per_epoch = len(converter_train) // batch_size\n",
    "        train_dataloader_iter = iter(train_dataloader)\n",
    "        for e in range(epoch_nums):\n",
    "            epoch_loss = train_one_epoch(model, train_dataloader_iter, steps_per_epoch, optimizer, loss_fn)\n",
    "            print('Epoch [{}/{}], Training Loss: {:.4f}'.format(e + 1, epoch_nums, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cde716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distributed(use_gpu):\n",
    "    backend = \"nccl\" if use_gpu else \"gloo\"\n",
    "    dist.init_process_group(backend)\n",
    "    device = int(os.environ[\"LOCAL_RANK\"]) if use_gpu  else \"cpu\"\n",
    "    model = SatCNN(13, 64, 64, 10).to(device)\n",
    "    model = DDP(model)\n",
    "    \n",
    "    output = train_model(model, device)\n",
    "    #dist.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063f46d",
   "metadata": {},
   "source": [
    "## Start Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a74e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributor = TorchDistributor(num_processes=6, local_mode=True, use_gpu=False)\n",
    "distributor.run(train_distributed, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
