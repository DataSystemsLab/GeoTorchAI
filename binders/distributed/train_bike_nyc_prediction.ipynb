{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dee2483",
   "metadata": {},
   "source": [
    "# This Example shows the Prediction of Bike Flow in the NYC City using the deep learning model ST_ResNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1cee6",
   "metadata": {},
   "source": [
    "Find the details of the ST-ResNet model in the <a href=\"https://dl.acm.org/doi/10.5555/3298239.3298479\">corresponding paper</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8ec05",
   "metadata": {},
   "source": [
    "Details of the dataset can be found <a href=\"https://github.com/FIBLAB/DeepSTN\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6762142",
   "metadata": {},
   "source": [
    "### Import Modules and Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f799943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from geotorchai.models.grid import STResNet\n",
    "from geotorchai.datasets.grid import BikeNYCDeepSTN\n",
    "from geotorchai.utility import TorchAdapter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Apache Sedona\n",
    "from sedona.spark import *\n",
    "\n",
    "## Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "## Import distributed modules\n",
    "from torch.utils.data import DistributedSampler, DataLoader\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "\n",
    "import warnings\n",
    "# Ignore FutureWarning warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ceca29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 19:04:54 WARN Utils: Your hostname, Kanchans-Laptop.local resolves to a loopback address: 127.0.0.1; using 192.168.1.6 instead (on interface en0)\n",
      "23/08/11 19:04:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/kanchan/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/kanchan/.ivy2/jars\n",
      "org.apache.sedona#sedona-spark-shaded-3.4_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1d2e36c3-45b5-4b04-a07d-2b25693d9e52;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/kanchan/.pyenv/versions/3.11.0/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.sedona#sedona-spark-shaded-3.4_2.12;1.4.1 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.4.0-28.2 in central\n",
      ":: resolution report :: resolve 85ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.sedona#sedona-spark-shaded-3.4_2.12;1.4.1 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.4.0-28.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1d2e36c3-45b5-4b04-a07d-2b25693d9e52\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n",
      "23/08/11 19:04:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/11 19:04:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/08/11 19:04:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "config = SedonaContext.builder().master(MASTER_URL).config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.4_2.12:1.4.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.4.0-28.2').getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae751d",
   "metadata": {},
   "source": [
    "## Get Path Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba670941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PATH_PREFIX= str(Path.home()) + '/' if os.environ.get('ENV_WB', 'false') == 'true' else ''\n",
    "\n",
    "print(PATH_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75031282",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29ba9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0002\n",
    "batch_size = 32\n",
    "\n",
    "PATH_TO_DATASET = PATH_PREFIX + \"data/deepstn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed609405",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbc3cdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloading started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 17708640/17708640 [00:00<00:00, 36660662.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloading finished\n",
      "File downloading started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 18224/18224 [00:00<00:00, 5532498.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloading finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Load training and test dataset\n",
    "full_dataset = BikeNYCDeepSTN(root = PATH_TO_DATASET, download = True)\n",
    "\n",
    "## get the min-max-difference of normalized data for future use in calculating actual losses\n",
    "min_max_diff = full_dataset.get_min_max_difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c026f70",
   "metadata": {},
   "source": [
    "### Method to Return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b021059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    len_closeness = 3\n",
    "    len_period = 4\n",
    "    len_trend = 4\n",
    "    nb_residual_unit = 4\n",
    "    map_height, map_width = 21, 12\n",
    "    nb_flow = 2\n",
    "    \n",
    "    ## Define Model\n",
    "    model = STResNet((len_closeness, nb_flow, map_height, map_width),\n",
    "                 (len_period, nb_flow, map_height, map_width),\n",
    "                 (len_trend, nb_flow, map_height, map_width),\n",
    "                 external_dim=None, nb_residual_unit=nb_residual_unit)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732ea6c3",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "Error will be high since the training is performed only for 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f504213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        X_c = sample[\"x_closeness\"].type(torch.FloatTensor).to(device)\n",
    "        X_p = sample[\"x_period\"].type(torch.FloatTensor).to(device)\n",
    "        X_t = sample[\"x_trend\"].type(torch.FloatTensor).to(device)\n",
    "        Y_batch = sample[\"y_data\"].type(torch.FloatTensor).to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_c, X_p, X_t)\n",
    "            loss = loss_fn(outputs, Y_batch)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3089cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, device):\n",
    "    ## Define hyper-parameters\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for e in range(epoch_nums):\n",
    "        epoch_loss = train_one_epoch(model, loader, optimizer, loss_fn, device)\n",
    "        print('Epoch [{}/{}], Training Loss: {:.4f}'.format(e + 1, epoch_nums, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75373fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distributed(use_gpu):\n",
    "    backend = \"nccl\" if use_gpu else \"gloo\"\n",
    "    dist.init_process_group(backend)\n",
    "    device = int(os.environ[\"LOCAL_RANK\"]) if use_gpu  else \"cpu\"\n",
    "    model = get_model().to(device)\n",
    "    model_ddp = DDP(model)\n",
    "    sampler = DistributedSampler(full_data)\n",
    "    loader = DataLoader(full_data, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "    train_model(model_ddp, loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219d4a1",
   "metadata": {},
   "source": [
    "## Start Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa50940",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training\")\n",
    "distributor = TorchDistributor(num_processes=2, local_mode=True, use_gpu=False)\n",
    "distributor.run(train_distributed, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
